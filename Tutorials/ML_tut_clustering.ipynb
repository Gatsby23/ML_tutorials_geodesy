{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_tut_clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aJTNlSt1u-BX",
        "tCI9GAcItNNt",
        "MNibOxz8tNjt",
        "eXthQ4kJvMwl",
        "qikizMiivSef",
        "7gAtLvoOvS1v"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrzIrOPstJYz"
      },
      "source": [
        "# Clustering Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUQMla0RtLpS"
      },
      "source": [
        "\n",
        "The goal of this script is to illustrate k-means clustering on bias and variance\n",
        "on measurements with time dependent noise. Based on these two features, different noise regimes are identified (e.g. meteorological situations for total station measurements).\n",
        "\n",
        "\n",
        "For this, do the following:\n",
        "\n",
        "    1. Definitions and imports\n",
        "    2. Simulate noise (Temperature T and Pressure P)\n",
        "    3. Simulate time series of measurements\n",
        "    4. Initialize the clustering\n",
        "    5. Execute k-means clustering\n",
        "    6. Plots and illustrations\n",
        "\n",
        "The following entries may be changed to explore the script:\n",
        "\n",
        "    * var_T_P             : The variance of temperature and pressure used to simulate the measurements.\n",
        "                          Increasing it increases the long and short - term variability.\n",
        "    * var_change_induced  : Additional variance of measurements occuring during periods of change.\n",
        "                          Increase to make measurements during transition periods (morning, evening) more uncertain.\n",
        "    * n_clusters          : The numbers of clusters to be found by the k-means clustering algorithm\n",
        "                                        \n",
        "This script is given out as part of the Machine Learning Tutorial during IV2020, Munich. Please consult the slides for some background information regarding motivation and a possible physical setup for this toy example.\n",
        "\n",
        "Written by Jemil Butt, Zan Gojcic, ETH Zurich."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJTNlSt1u-BX"
      },
      "source": [
        "# 1. Definitions and imports ------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32JUQgyop5ft"
      },
      "source": [
        "We suppose that the total station executes $10$ measurements per hour leading to $n_{timesteps}=240$ measurements per day. At each of the times of measurement $t_k, k=1, ... ,n_{timesteps}$ the meteorological conditions (in form of temperature and pressure) affect the measurements leading random and systematic deviations in the measured distances. Timeseries data will be generated for $n_{episodes}$ different days.\n",
        "\n",
        "Define the variabilities of $T$ and $P$ by specifying their variances $\\sigma_T^2, \\sigma_P^2$. Additional volatility occurs during periods of significant drifts of temperature or pressure. The size of this effect is quantified by $\\sigma^2_{change ~induced}$. The amount of recognizably different situations suspected to be found in the data can be specified by the user by changing the value of $n_{clusters}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-nZrS00tNDP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e696a230-2129-402c-86bc-4805d0cafd97"
      },
      "source": [
        "\n",
        "# 1.1 Import numerical and plotting libraries\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import matlib as mb\n",
        "\n",
        "\n",
        "# 1.2 Define the number of datapoints (measurements per day) gathered and the number of episodes (days)\n",
        "n_timesteps=240                             # Needs to be divisible by 20\n",
        "n_episodes=20\n",
        "\n",
        "time=np.linspace(0,1,n_timesteps)\n",
        "time_plot=np.linspace(0,24,n_timesteps)\n",
        "\n",
        "# 1.3 Parameters for simulation and clustering    # The following parameters may be changed to explore the method\n",
        "var_T_P=np.array([0.5,5])                         # Variance of temperature [C], pressure [hPA]. Increase for higher long\n",
        "                                                  # long term volatility\n",
        "var_change_induced=5                              # Increase for higher noise during periods of changes\n",
        "n_clusters=3                                      # Numbers of clusters to be discovered\n",
        "\n",
        "print('Packages imported. The parameters for simulation and clustering have been defined and are now \\\n",
        "ready for later use.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Packages imported. The parameters for simulation and clustering have been defined and are now ready for later use.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCI9GAcItNNt"
      },
      "source": [
        "#2. Simulate noise (Temperature T and Pressure P) --------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZIegczWp66Y"
      },
      "source": [
        "Simulate the evolution of temperature $T$ and pressure $P$ over $n_{episodes}$ different days. For this, assume both quantities to vary randomly in time but in an autocorrelated way - each realization is a random walk starting and ending at 0. The expected value of the temperature $T$ peaks during noon and has a minimum during the night whereas there are no time dependent drifts simulated for the evolution of the pressure $P$. $T$ and $P$ are more volatile during the morning and the evening reflected in higher variances during those times. Some simulations of temperature and pressure can be seen in the images below.\n",
        "\n",
        "Example temperature $\\hspace{3cm}$ All temperature evolutions $\\hspace{3cm}$ Example pressure $\\hspace{3cm}$ All pressure evolutions \n",
        "![alt text](https://drive.google.com/uc?id=1KV49KtbKMieN7EBJfPV1xqb21QRQLSI2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vAAhefatNYB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "27f77791-4996-479f-d866-3f7584003c72"
      },
      "source": [
        "\n",
        "# 2.1 Define the mean vectors of the simulations\n",
        "\n",
        "bump_fun=np.concatenate((np.linspace(0,0,int(n_timesteps/6)),np.linspace(0,1,int(n_timesteps/6)),\\\n",
        "                         np.linspace(1,1,int(n_timesteps/3)),np.linspace(1,0,int(n_timesteps/6)),np.linspace(0,0,int(n_timesteps/6))))\n",
        "\n",
        "mean_vector_T=10*np.ones(n_timesteps)+3*bump_fun\n",
        "mean_vector_P=1013.5*np.ones(n_timesteps)\n",
        "\n",
        "# 2.2 Define the covariance matrices of the simulations \n",
        "Cov_mat_T=np.zeros([n_timesteps,n_timesteps])\n",
        "Cov_mat_P=np.zeros([n_timesteps,n_timesteps])\n",
        "\n",
        "for i in range(n_timesteps):\n",
        "    for j in range(n_timesteps):\n",
        "        Cov_mat_T[i,j]=var_T_P[0]*(min(time[i],time[j])-time[i]*time[j])\n",
        "        Cov_mat_P[i,j]=var_T_P[1]*(min(time[i],time[j])-time[i]*time[j])\n",
        "\n",
        "# Add change dependent noise onto the Brownian bridge covariance\n",
        "Change_induced_noise=np.diag(var_change_induced*np.power(mean_vector_T-np.roll(mean_vector_T,1),2))\n",
        "Cov_mat_T=Cov_mat_T+Change_induced_noise\n",
        "\n",
        "# 2.3 Execute the simulations of temperature and Pressure\n",
        "T_over_time=np.zeros([n_timesteps,n_episodes])\n",
        "P_over_time=np.zeros([n_timesteps,n_episodes])\n",
        "\n",
        "for k in range(n_episodes):\n",
        "    T_over_time[:,k]=np.random.multivariate_normal(mean_vector_T,Cov_mat_T)\n",
        "    P_over_time[:,k]=np.random.multivariate_normal(mean_vector_P,Cov_mat_P)\n",
        "\n",
        "print('Covariance matrices have been constructed. Simulations were carried out by sampling from \\\n",
        "multivariate normal distributions.')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Covariance matrices have been constructed. Simulations were carried out by sampling from multivariate normal distributions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNibOxz8tNjt"
      },
      "source": [
        "# 3.Simulate time series of measurements ------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irVPWcPTp7rT"
      },
      "source": [
        "Use the timeseries of $T$ and $P$ to calculate the bias in the distance measurements $d$ by using the standard equation\n",
        "\n",
        "$$ \\Delta d = [\\Delta T -0.3 \\Delta P] d  $$\n",
        "\n",
        "to calculate timeseries of distance biases $\\Delta d (t)$ for each day. From $\\Delta d(t)$ extract timeseries of biases $\\mu(t)$ and variances $\\sigma^2(t)$.\n",
        "They will act as the features that will be clustered later on. Each feature vector $x_k=(\\mu(t_k),\\sigma^2(t_k)$ is a point in a two-dimensional plane and the goal of the clustering is to identify regimes with similar stochastic properties. The features are centered and whitened to ensure that the Euclidean distance between them is a meaningful measure of closeness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxZMMEnCtNs4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16aece12-5b2e-49f9-9817-f61c4c060bb6"
      },
      "source": [
        "# 3.1 Process temperature and pressure to generate changes of distance measurements\n",
        "T_0=T_over_time[0]\n",
        "P_0=P_over_time[0]\n",
        "\n",
        "Delta_T=T_over_time-T_0\n",
        "Delta_P=P_over_time-P_0\n",
        "Delta_d_measured=-(-Delta_T+0.3*Delta_P)*0.1\n",
        "\n",
        "# 3.2 Calculate estimates of biases and variances as features for clustering\n",
        "n_aggregation=20\n",
        "bias=[]\n",
        "variance=[]\n",
        "time_datapoint=[]\n",
        "\n",
        "for k in range(n_episodes):\n",
        "    Delta_d=np.reshape(Delta_d_measured[:,k],[int(n_timesteps/n_aggregation),n_aggregation])\n",
        "    Delta_d=Delta_d.T\n",
        "    bias=np.append(bias,np.mean(Delta_d,0))\n",
        "    variance=np.append(variance,np.std(Delta_d,0))\n",
        "    time_datapoint=np.append(time_datapoint,np.mean(np.reshape(time_plot,[int(n_timesteps/n_aggregation),n_aggregation]),1))\n",
        "     \n",
        "bv_mat=np.vstack([bias,variance])\n",
        "\n",
        "# 3.3 Whiten the data\n",
        "n_datapoints=len(bias)\n",
        "bv_mat_unwhitened=copy.copy(bv_mat)\n",
        "bv_mean=np.reshape(np.mean(bv_mat,1),(2,1));\n",
        "bv_corr=(1/n_datapoints)*(bv_mat@(bv_mat.T))\n",
        "u,s,vh=np.linalg.svd(bv_corr)\n",
        "S_whiten=u@np.diag(np.divide(1,np.sqrt(s)))@vh\n",
        "bv_mat=S_whiten@(bv_mat-mb.repmat(bv_mean,1,n_datapoints))\n",
        "\n",
        "print('Measurements constructed from T and P data. Bias and variance features extracted and whitened.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Measurements constructed from T and P data. Bias and variance features extracted and whitened.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXthQ4kJvMwl"
      },
      "source": [
        "# 4. Initialize the clustering ----------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpnmHIrgp8sn"
      },
      "source": [
        "The classical k-means clustering aims to find a set of $k$ clustercenters in such a way that the intra-group variances in the $k$ clusters are maximally small. The mathematical formulation of this is as an optimization problem for the clustercenters $\\bar{x}_k, k=1, ... n_{clusters}$ of the form\n",
        "\n",
        "$$ \\bar{x}_1, ... ,\\bar{x}_{n_{clusters}} = \\underset{v_1, ... v_{n_{clusters} \\subset \\mathbb{R^{n_{features}}}}}{\\operatorname{argmin}} \\sum_{S_i \\in Clusters}\\sum_{x \\in S_i} \\|x-v_i\\|^2_2. $$\n",
        "\n",
        "The k-means algorithm does this in two steps: First, the clustercenters are initialized randomly. Then an iterative procedure is applied that shifts the clustercenters to minimize intra-group variance. The initialization is done in a way that the initial clustercenters are preferrably far away from each other to increase the chance of the initial clustercenters already lying in different classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKZYZLkws_Zi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "080db20c-8d5e-435b-e34e-452a0b412e99"
      },
      "source": [
        "# 4.1 Choose first center uniformly then use k++ method\n",
        "index=np.zeros(n_clusters)\n",
        "initial_clustercenters=np.zeros((2,n_clusters))\n",
        "\n",
        "index[0]=np.random.permutation(range(n_datapoints))[0]\n",
        "initial_clustercenters[:,0]=bv_mat[:,int(index[0])]\n",
        "\n",
        "# 4.2 Choose next center with probability proportional to dist^2\n",
        "D_mat=np.zeros((n_datapoints,n_clusters))\n",
        "for k in range(n_clusters):\n",
        "    icc_k=initial_clustercenters[:,k]\n",
        "    icc_k=icc_k.reshape(2,1)\n",
        "    Diff_mat=bv_mat-mb.repmat(icc_k,1,n_datapoints)\n",
        "    D_mat[:,k]=np.linalg.norm(Diff_mat,axis=0)\n",
        "    D_to_closest=np.min(D_mat[:,0:k+1],axis=1)\n",
        "    P_mat=np.power(D_to_closest,2)\n",
        "    P_mat=np.divide(P_mat,np.sum(P_mat))\n",
        "    index_next_cc=np.random.choice(n_datapoints,p=P_mat)\n",
        "    if k<n_clusters-1:\n",
        "        initial_clustercenters[:,k+1]=bv_mat[:,index_next_cc]\n",
        "\n",
        "print('Clustercenters initialized randomly by the k++ method.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clustercenters initialized randomly by the k++ method.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qikizMiivSef"
      },
      "source": [
        "# 5. Execute k-means clustering ---------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p21w2ADFp9YJ"
      },
      "source": [
        "The iterative procedure consists of a sequence of shifts in the clustercenters $\\bar{x}_1, ... \\bar{x}_{n_{Clusters}}$ followed by a recalculation of which points belong to which center based on minimum Euclidean distances. The pseudocode is\n",
        "$$ \\begin{align} \\text{while}& \\underset{j\\in \\{1, ... ,n_{clusters}\\}}{max}\\|\\bar{x}_j^{new}-\\bar{x}_j^{old}\\|_2>10^{-6} \\\\ \\text{do} & \\text{ for each } x\\in \\text{ featurevectors} \\\\\n",
        "& class(x) =class\\left( \\underset{\\bar{x}_j^{old}\\in Clustercenters}{\\operatorname{argmin}} \\|\\bar{x}_j^{old} - x\\|_2\\right)\\\\\n",
        "& \\bar{x}_j^{new}=\\frac{1}{|S_j|}\\sum_{x \\in S_j}x,\\hspace{1cm}\n",
        " \\bar{x}_j^{old}=\\bar{x}_j^{new} \\hspace{1cm} \\text{ for } j=1, ... ,n_{clusters}\n",
        "\\end{align}$$\n",
        "A typical sequence of refinement steps can be seen in the sequence of images below.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1pmvcj8Jxucab3beBKHanWB6C5UsZ4u2n)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne5UhJ-2vSrS"
      },
      "source": [
        "# 5.1 Iteratively refine using standard k-means (Lloyds algorithm)\n",
        "Delta=1\n",
        "Centers=copy.copy(initial_clustercenters)\n",
        "Delta_vec=np.zeros((n_clusters,1))\n",
        "\n",
        "while Delta>=np.power(10.0,-6):\n",
        "    # Assignment step\n",
        "    classes=np.argmin(D_mat,axis=1)\n",
        "    # Update step\n",
        "    for k in range(n_clusters):\n",
        "        points_cluster_k=bv_mat[:,classes==k]\n",
        "        Delta_vec[k]=np.linalg.norm(Centers[:,k]-np.mean(points_cluster_k,1))\n",
        "        Centers[:,k]=np.mean(points_cluster_k,1)\n",
        "        Center_k=Centers[:,k]\n",
        "        Center_k=Center_k.reshape(2,1)\n",
        "        Diff_mat=bv_mat-mb.repmat(Center_k,1,n_datapoints)\n",
        "        D_mat[:,k]=np.linalg.norm(Diff_mat,axis=0)\n",
        "    # Residual evaluation\n",
        "    Delta=np.max(Delta_vec)\n",
        "\n",
        "Centers_unwhitened=np.linalg.pinv(S_whiten)@Centers+mb.repmat(bv_mean,1,n_clusters)\n",
        "initial_clustercenters_unwhitened=np.linalg.pinv(S_whiten)@initial_clustercenters+mb.repmat(bv_mean,1,n_clusters)\n",
        "\n",
        "print('K-means clustering iterations performed till convergence.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gAtLvoOvS1v"
      },
      "source": [
        "# 6. Plots and illustrations ------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oapEXwbKp-BF"
      },
      "source": [
        "We illustrate the procedure and its results by plotting in figure one the input data in form of timeseries of distance biases $\\Delta d(\\cdot)$. They are converted to sequences of feature vectors $x_k=(\\mu(t_k),\\sigma^2(t_k))$ indexed by time $t_k$. The totality of those feature vectors is ploted in the second image.\n",
        "\n",
        "The third and fourth image show the results of the clustering algorithm. After a sequence of refinement steps, three clusters are indentified: \n",
        "* one corresponding to low bias and low variance (measurements in the night)\n",
        "* one corresponding to high bias and low variance (measurements during noon)\n",
        "* one corresponding to medium bias and high variance (measurements during dawn and dusk)\n",
        "\n",
        "The membership of the feature vectors is symbolized by different colours and associating the feature vectors with their temporal coordinates allows to also colour the original timeseries of distance biases $\\Delta d$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_pi_r8RvS-6"
      },
      "source": [
        "\n",
        "# Figure 1 showing sample time series of measurements from several days\n",
        "plt.figure(1)\n",
        "plt.plot(time_plot,Delta_d_measured)\n",
        "plt.title('Time series of measurements (several days)')\n",
        "plt.xlabel('Time in  h')\n",
        "plt.ylabel('Measured values')\n",
        "\n",
        "# Figure 2 showing bias and variance of segments of the time series. Bias is \n",
        "# calculated as average deviation from 0. n_aggregation measurements form a segment.\n",
        "plt.figure(2)\n",
        "plt.scatter(bias,variance)\n",
        "plt.scatter(initial_clustercenters_unwhitened[0,:],initial_clustercenters_unwhitened[1,:],c='r')\n",
        "plt.title('Feature plot describing segments of time series')\n",
        "plt.xlabel('Bias')\n",
        "plt.ylabel('Variance')\n",
        "plt.legend(['Observed features','Initial clustercenters'])\n",
        "\n",
        "# Figure 3 showing the output of the clustering algorithm. Points of different\n",
        "# colors are associated to different clusters. The black dots are the clustercenters.\n",
        "plt.figure(3)\n",
        "for k in range(n_clusters):\n",
        "    plt.scatter(bv_mat_unwhitened[0,classes==k],bv_mat_unwhitened[1,classes==k])\n",
        "plt.scatter(Centers_unwhitened[0,:],Centers_unwhitened[1,:],c='k')\n",
        "plt.title('Output of clustering algorithm')\n",
        "plt.xlabel('Bias')\n",
        "plt.ylabel('Variance')\n",
        "legend_3=[]\n",
        "for k in range(n_clusters):\n",
        "    legend_3.append('Cluster %d' % int(k+1))\n",
        "legend_3.append('Clustercenters')\n",
        "plt.legend(legend_3)\n",
        "\n",
        "# Figure 4 showing into which cluster each segment of the several time series\n",
        "# is grouped. Note that the different stochastic behavior at different times of \n",
        "# day is reflected in the clustering.\n",
        "plt.figure(4)\n",
        "for k in range(n_clusters):\n",
        "    plt.scatter(time_datapoint[classes==k],bv_mat_unwhitened[0,classes==k])\n",
        "plt.title('Clustering of segments of time series')\n",
        "plt.xlabel('Time in h')\n",
        "plt.ylabel('Measurement values')\n",
        "legend_4=[]\n",
        "for k in range(n_clusters):\n",
        "    legend_4.append('Cluster %d' % int(k+1))\n",
        "plt.legend(legend_4)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}