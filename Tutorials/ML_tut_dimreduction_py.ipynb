{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_tut_dimreduction.py.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wau_7tVcmmCY",
        "Q0_GEZ9hxUem",
        "h98uGxPkmuWf",
        "HO71ONMNmvZv",
        "PokHm1Iqm5_a"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqzWox9dlk08"
      },
      "source": [
        "# Dimensionality reduction example\n",
        "\n",
        "The goal of this script is to illustrate dimensionality reduction of timeseries\n",
        "data using the singular value decomposition to derive a maximum variance \n",
        "representation.\n",
        "\n",
        "For this, do the following:\n",
        "\n",
        "    1. Definitions and imports\n",
        "    2. Create covariance function\n",
        "    3. Simulate the data\n",
        "    4. Dimensionality reduction\n",
        "    5. Plots and illustrations\n",
        "\n",
        "The following entries may be changed to explore the script:\n",
        "\n",
        "    * sigma_noise , sigma_signal  : Both together define the signal-to-noise ration in the data\n",
        "    * cov_type                    : The form of the correlation function. Only the strings 'sq_exp',\n",
        "                                    'exp', and 'white' are allowed.\n",
        "    * corr_length_signal          : A quantity describing how slow the correlation between values drops\n",
        "                                    depending on temporal separation. Increase to generate a smoother signal.\n",
        "    * cov_type_guess              : The form of the guessed correlation function. Only the strings 'sq_exp',\n",
        "                                    'exp', and 'white' are allowed.\n",
        "    * corr_length_signal_guess    : A quantity describing how slow assumedly the correlation between values drops\n",
        "                                    depending on temporal separation. Increase to generate a smoother signal.                                                                      \n",
        "    * n_expansion                 : The maximum amount of expansion coefficients for the dimensionality reduction.\n",
        "                                    The bigger this quantity, the more terms are used to represent the signal.\n",
        "    * n_show                      : The amount of reconstructions / elements shown in figures\n",
        "                                                                        \n",
        "This script is given out as part of the Machine Learning Tutorial during IV2020, Munich. Please consult the slides for some background information regarding motivation and a possible physical setup for this toy example.\n",
        "\n",
        "Written by Jemil Butt, Zan Gojcic, ETH Zurich."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wau_7tVcmmCY"
      },
      "source": [
        "# 1. Definitions and imports ------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzX2jLruKNks"
      },
      "source": [
        "Define the length $n_{datapoints}$ of the timeseries $x$, the standarddeviation of noise $\\sigma_{noise}$ and the correlation length $\\rho_{signal}$ of the simulated signal. Different correlation lengths $\\rho_{signal}$ lead to different regularity properties of the data that is to be reconstructed in later steps. Different types of correlation structures can be chosen.\n",
        " $$\\begin{align}\\text{'sq_exp'}: k_{signal}(s,t)&=\\exp\\left[-\\left(\\frac{s-t}{\\rho_{signal}}\\right)^2\\right] &&\\text{leads to smooth signals} \\\\\n",
        " \\text{'exp'}: k_{signal}(s,t)&=\\exp\\left[-\\frac{|s-t|}{\\rho_{signal}}\\right] &&\\text{leads to continuous, nondifferentiable signals}\\\\\n",
        "   \\text{'white'}: k_{signal}(s,t)&=\\delta_{st} =\\mathbb{1}_{s=t}(s,t) &&\\text{leads to uncorrelated signals} \\end{align}$$\n",
        "\n",
        "Example signal 'sq_exp'       $\\hspace{5cm}$     Example signal 'exp' $\\hspace{5cm}$ Example signal 'white'\n",
        "\n",
        "![High correlation length -> smooth behavior](https://drive.google.com/uc?id=1tWeF_Btm5L-pMf0sk2ZNipUnNWwUeCiw)\n",
        "\n",
        "Adjusting $\\sigma_{noise}$ leads to different signal to noise ratios. The bigger $\\sigma_{noise}$ is the more noisy the data looks like and the harder it is for the algorithm to reconstruct the data due to the unpredictability of the white noise. For reconstructing the data, $n_{expansion}$ basis functions $u_k(\\cdot)$ are employed. They are superimposed in the following way.\n",
        "$$ x_{data}\\approx \\sum_{k=1}^{n_{expansion}}\\alpha_k u_{k}(\\cdot)$$ The number n_{expansion} of basis functions to be used is adjustable and the quality of the reconstruction improves with increasing $n_{expansion}$. The basis functions $u_k(\\cdot)$ are not known a priori and finding them so as to reconstruct $x_{data}$ with the minimum number of coefficients is part of the problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWvEWKedkO5O"
      },
      "source": [
        "\n",
        "# 1.1 Import numerical and plotting libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 1.2 Define the number of observations and the time at which they take place\n",
        "#       (Time interval is arbitrarily set to [0,1] to avoid scaling issues)\n",
        "n_datapoints=100\n",
        "time=np.linspace(0,1,n_datapoints)\n",
        "\n",
        "#1.3 Parameters for simulationa and estimation\n",
        "\n",
        "# Parameters for simulation             # The following parameters may be changed to explore the method\n",
        "sigma_noise=0.0                         # Quantifies the average magnitude of the noise\n",
        "cov_type='sq_exp'                       # Quantifies the correlation structure of the data\n",
        "corr_length_signal=0.3                  # A quantity describing how slow the correlation between values drops\n",
        "                                        # depending on temporal separation.\n",
        "                                        \n",
        "# Parameters for dimensionality reduction    \n",
        "cov_type_guess='sq_exp'                 # A guess for the correlation structure of the data\n",
        "corr_length_signal_guess=0.3            # A guess for the smoothness of the signal. Is used to construct a \n",
        "                                        # covariance matrix whose eigenfunctions reconstruct the signal                                                  \n",
        "n_expansion=10                          # The amount of eigenfunctions used to reconstruct the measured signal\n",
        "\n",
        "# Parameters for visualization\n",
        "n_show=3\n",
        "\n",
        "print('Packages imported. The parameters for simulation and representation have been defined and are now \\\n",
        "ready for later use.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0_GEZ9hxUem"
      },
      "source": [
        "# 2. Create covariance function  ------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whWzMetZKOBI"
      },
      "source": [
        "Covariance functions $k(\\cdot,\\cdot)$ are a compact way of encoding the spatial dependence of the correlation structure. They are used to construct the covariance matrices necessary for inference and prediction by evaluating $k$ via\n",
        "$$k(\\cdot,\\cdot):T\\times T\\ni (s,t)\\mapsto k(s,t)\\in \\mathbb{R}$$\n",
        "to generate the covariance matrix $K$ with elements $(K)_{ij}=k(t_i,t_j)$. For the simple case of an 1-D interval $T$ in time, $k(\\cdot,\\cdot)$ is a two-dimensional function specifying the correlation between $t$ and $s$ for all $(s,t)\\in T\\times T$. Some typical structures are plotted below.\n",
        "\n",
        "Covariance matrix 'sq_exp'       $\\hspace{3cm}$     Covariance matrix 'exp' $\\hspace{3cm}$ Covariance matrix 'white'\n",
        "![alt text](https://drive.google.com/uc?id=1GpVAyEaOwiWonGdTSO5tmmeGTZopClWI)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZULFU5A6yJ81"
      },
      "source": [
        "\n",
        "def Create_covariance_matrix(cov_type,input_vector,correlation_length,dimensions):\n",
        "\n",
        "    # 2.1 Rename for comfort   \n",
        "    n=dimensions\n",
        "    t=input_vector\n",
        "\n",
        "    # 2.2 Case based evaluation \n",
        "\n",
        "    if cov_type == 'white':\n",
        "        cov_fun=lambda t_1,t_2: np.multiply(1,t_1==t_2)\n",
        "    elif cov_type == 'sq_exp':\n",
        "        cov_fun=lambda t_1,t_2: np.exp(-np.power(((t_1-t_2)/correlation_length),2))\n",
        "    elif cov_type == 'exp':\n",
        "        cov_fun=lambda t_1,t_2: np.exp(-np.abs(((t_1-t_2)/correlation_length)))\n",
        "    else:\n",
        "        print('Wrong type of covariance function specified.')\n",
        "        \n",
        "    # 2.3 Assemble matrix and return \n",
        "\n",
        "    Cov_mat=np.zeros((n,n))\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            Cov_mat[i,j]=cov_fun(t[i],t[j])\n",
        "            \n",
        "    return Cov_mat\n",
        "\n",
        "print('Defined a function for assembling covariance matrices from correlation functions.')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h98uGxPkmuWf"
      },
      "source": [
        "# 3. Simulate the data ------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GlGckxNKOn-"
      },
      "source": [
        "Simulate randomly chosen smooth signals by considering them to be drawn from a multivariate Gaussian distribution with dimension $n_{timesteps}$. With e.g. $n_{timesteps}=100$, each throw of the dice produces a vector of length 100 that we interpret as a function mapping dimension indices to function values. We then draw $x_{signal}$ according to\n",
        "$$ x_{signal}\\sim\\mathcal{N}(0,K)$$\n",
        "where $0\\in \\mathbb{R}^{100}$ and $K\\in \\mathbb{R}^{100 \\times 100}$. $\\mathcal{N}$ symbolizes the multivariate Gaussian distribution. Even though each draw is random and the different realizations are independent of each other, there exists significant spatial correlation between neighboring values. This can be exploited not only for interpolation and estimation (as done in the regression example). It also to means that of the $100$ elements few are truly uncorrelated and many redundant - therefore the $100$ elements in $x_{signal}$ can be represented by significantly less numbers. More explanations of the intuition behind the simulation process may be found in the notes associated to the regression example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcL6Xhr8mvOD"
      },
      "source": [
        "\n",
        "# 3.1 Define the covariance matrices of signal an noise for simulation\n",
        "\n",
        "Cov_mat_signal=Create_covariance_matrix(cov_type,time,corr_length_signal,n_datapoints)\n",
        "Cov_mat_noise=pow(sigma_noise,2)*Create_covariance_matrix('white',time,0,n_datapoints)\n",
        "\n",
        "# 3.2 Define a guess for the covariance matrix of the signal\n",
        "Cov_mat_signal_guess=Create_covariance_matrix(cov_type_guess,time,corr_length_signal_guess,n_datapoints)\n",
        "        \n",
        "# 3.3 Do the simulation - first simulate random signal ...\n",
        "x_true=np.random.multivariate_normal(np.zeros([n_datapoints]),Cov_mat_signal)\n",
        "\n",
        "# ... then add noise to create the synthetic measurements\n",
        "x_measured=x_true+np.random.multivariate_normal(np.zeros([n_datapoints]),Cov_mat_noise)\n",
        "\n",
        "print('Covariance matrices have been constructed. Simulations were carried out by sampling from \\\n",
        "multivariate normal distributions.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO71ONMNmvZv"
      },
      "source": [
        "# 4. Dimensionality reduction -----------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byMtczXYKPWM"
      },
      "source": [
        "The goal is to find an orthonormal sequence of basis vectors $\\{u_j(\\cdot)\\}_{j=1}^k$ such that reconstruction of the signal $x_{signal}$ via $x_{reconstruct}=\\sum_{j=1}^k\\langle x_{signal},u_j\\rangle u_j$ is possible up to a small residual RMSE with the smallest number $k$ of basis vectors. Since our data is stochastic and the exact form of $x_{signal}$ is unknown beforehand, we need to pose the optimization problem in terms of expected values. The task is to solve\n",
        "$$ u_1, ... , u_k =\\underset{v_1, ... , v_k \\text{ orthonormal}}{\\operatorname{argmin}} E\\left[ \\|\\sum_{j=1}^k \\langle x_{signal},v_j\\rangle v_j - x_{signal}\\|\\right]. $$\n",
        "\n",
        "Knowledge of the covariance matrix $K$ is sufficient to reconstruct any randomly chosen signal $x_{signal}$ with known correlation structure and in this case a closed form solution can be derived. It is based on the spectral decomposition of $K$.\n",
        "\n",
        "$$\\text{Spectral theorem } \\hspace{2cm} K= U \\Lambda U^T \\hspace{2cm}\\text{where } \\Lambda \\text{ diagonal and } U \\text{ unitary}$$\n",
        "\n",
        "\n",
        "Denoting the elements of $\\Lambda$ by $\\lambda_i$ one finds the energy of the expected reconstruction error to be\n",
        "$$ \\begin{align}\n",
        "E[\\|x_{signal}-x_{reconstruct}\\|^2_2]=E\\left[\\|\\sum_{j=k+1}^{n_{timesteps}}\\langle x_{signal},u_j\\rangle u_j\\|^2_2\\right]&=\\sum_{j=k+1}^{n_{timesteps}}u_jE\\left[x_{signal}x_{signal}^T\\right]u_j \\\\\n",
        "&=\\sum_{j=k+1}^{n_{timesteps}} \\lambda_j\n",
        "\\end{align}$$\n",
        "\n",
        "The reconstruction error decreases therefore monotonically with $k$ and the first eigenvector decreases the energy of the residuals $x_{signal} - x_{reconstruct}$ by $\\lambda_1$ - the largest amount possible. This means that - on average - no other single function $u(\\cdot)$ can be used better to approximate $x_{signal}$ than $u_1(\\cdot)$.\n",
        "When trying to find the best $k$ vectors $v_1 , ... ,v_k$ to reconstruct $x_{signal}$ via $x_{reconstruct}=\\sum_{j=1}^k \\alpha_j v_j$, the eigenvectors $u_1, ... ,u_k$ minimize $E\\left[\\|x_{signal}-x_{reconstruct}\\|^2_2\\right]$ among all possible vectors and are therefore the best choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdldImU9mvjq"
      },
      "source": [
        "\n",
        "# 4.1 Extract basis elements\n",
        "u,s,vh=np.linalg.svd(Cov_mat_signal_guess)\n",
        "basis_elements=u[:,0:n_expansion]\n",
        "\n",
        "# 4.2 Represent signal in new basis\n",
        "coeffs=basis_elements.T@x_measured\n",
        "signal_reconstruct=np.zeros([n_datapoints,n_expansion])\n",
        "reconstruction_rmse=np.zeros(n_expansion)\n",
        "\n",
        "for k in range(n_expansion):\n",
        "    signal_reconstruct[:,k]=basis_elements[:,0:k+1]@coeffs[0:k+1]\n",
        "    reconstruction_rmse[k]=np.linalg.norm(signal_reconstruct[:,k]-x_measured)/np.sqrt(n_datapoints);\n",
        "\n",
        "print('Singular value decomposition performed and basis functions extracted. Reconstruction using the \\\n",
        "most efficient n_expansion basis functions.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PokHm1Iqm5_a"
      },
      "source": [
        "# 5. Plots and illustrations ------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJCY0VJ9KQJS"
      },
      "source": [
        "Illustrate the decomposition and reconstruction procedure by first plotting the randomly chosen signal to be processed. The eigenvectors corresponding to the largest eigenvalues of the covariance matrices are plotted in the second figure. They correspond to the basis vectors with the most explanatory power. \n",
        "\n",
        "Figure 3 shows the increasingly more faithful reconstructions whose RMSE monotonically decreases with increasing number of basis functions used for reconstruction. For smooth functions with much correlations structure to exploit, $10$ coefficients may be sufficient for an almost perfect reconstruction amounting to a compression of factor $10$. The actual decay of the RMSE with increasing amount of basis elements used for reconstruction is shown in figure 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0nNBWM4m6LH"
      },
      "source": [
        "\n",
        "# 5.1 Figure showing the measured data and the true underlying signal\n",
        "plt.figure(1)\n",
        "plt.scatter(time,x_measured,s=60, facecolors='none', edgecolors='k')\n",
        "plt.plot(time,x_true,color='xkcd:cerulean',linewidth=2)\n",
        "plt.title('True signal and measured data')\n",
        "plt.legend(['x_true','x_measured'])\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Function value')\n",
        "\n",
        "# 5.2 Figure showing the basis elements used to reconstruct the measured signal\n",
        "plt.figure(3)\n",
        "index_show=np.round(np.linspace(0,n_expansion-1,n_show)).astype(int)\n",
        "index_basis=index_show+np.ones(3)\n",
        "plt.plot(time,basis_elements[:,index_show])\n",
        "plt.title('Basis elements used for approximation')\n",
        "plt.legend(['Basis element %d'% index_basis[0],'Basis element %d'% index_basis[1], 'Basis element %d' % index_basis[2]])\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Function value')\n",
        "\n",
        "# 5.3 Figure showing the reconstructions of the measured data in terms of an \n",
        "# increasing sequence of basis elements\n",
        "plt.figure(2)\n",
        "plt.plot(time,signal_reconstruct[:,index_show])\n",
        "plt.title('Approximations to data using basis elements')\n",
        "plt.legend(['Approximation using %d elements' % index_basis[0],'Approximation using %d elements' % index_basis[1],\\\n",
        "            'Approximation using %d elements' % index_basis[2]])\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Function value')\n",
        "\n",
        "# 5.4 Figure showing the decay of the rmse with increasing amount of basis elements\n",
        "# used for reconstruction\n",
        "plt.figure(4)\n",
        "plt.plot(np.insert(reconstruction_rmse,0,np.linalg.norm(x_measured)/np.sqrt(n_datapoints)))\n",
        "plt.title('Root mean square error')\n",
        "plt.legend(['RMSE'])\n",
        "plt.xlabel('# Expansion coefficients')\n",
        "plt.ylabel('RMSE')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}